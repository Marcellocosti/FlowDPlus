savecfg: true

input: # files to use, set FD to null for binary classification
    prompt: /data/shared/DplFlowML/MLtrees/MC/Train306228/HF_LHC24g2_2P3PDstar_50100_PromptDplus_frac0.6.parquet
    FD: /data/shared/DplFlowML/MLtrees/MC/Train306228/HF_LHC24g2_2P3PDstar_50100_NonPromptDplus_frac0.6.parquet
    data: /data/shared/DplFlowML/MLtrees/Data/Train305807/HF_LHC23zzh_pass4_small_2P3PDstar_50100.parquet
    treename: null

output:
    leg_labels: # legend labels, keep the right number of classes
        Bkg: Background
        Prompt: Prompt D$^+$
        FD: Feed-down D$^+$
    out_labels: # output labels, keep the right number of classes
        Bkg: Bkg
        Prompt: Prompt
        FD: FD
    dir: '/home/mdicosta/flowDplus/MLtraining/50100/no_opt_chi2pca_fullsample_frac06/ncands' # output dir

pt_ranges: # ranges in pt to split the data in the ml training and testing
    # min: [1, 2, 4, 6, 8,  12] # list
    # max: [2, 4, 6, 8, 12, 24] # list
    min: [1, 3, 5, 8, 12] # list
    max: [3, 5, 8, 12, 24] # list

data_prep:
    filt_bkg_mass: (fM < 1.80 and fM > 1.72) or (fM > 1.95 and fM < 2.02) # pandas query to select bkg candidates
    dataset_opt: max_signal  # change how the dataset is built, options available: 'equal', 'max_signal'
                             # 'equal' -> same number of prompt/FD/bkg (not using all the signal available)
                             # 'max_signal' -> try to use all the signal (prompt and FD) + add n_bkg = 2 * (n_prompt + n_FD)
    bkg_mult: [5., 5., 5., 5., 5., 5.] # list of multipliers for (nPrompt + nFD) used to determine nCandBkg in the 'max_signal' option
    seed_split: 42 # seed used for train_test_split(...)
    test_fraction: 0.1 # fraction of data used for test set and efficiencies

ml:
    raw_output: False # use raw_output (True) of probability (False) as output of the model
    roc_auc_average: 'macro' # 'macro' or 'weighted'
    roc_auc_approach: 'ovo'  # 'ovo' or 'ovr'
    # training_columns: [ d_len_xy, norm_dl_xy, cos_p_xy, imp_par_xy, max_norm_d0d0exp, cos_t_star,imp_par_prod, nsigComb_Pi_0, nsigComb_K_0, nsigComb_Pi_1, nsigComb_K_1]
    #                    # list of training variables
    training_columns: [ 'fDecayLength', 'fDecayLengthXY', 'fCpa', 'fCpaXY', 
                        'fImpactParameter0', 'fImpactParameter1', 'fImpactParameter2', 
                        'fNSigTpcTofPi0',   'fNSigTpcTofKa1', 'fNSigTpcTofPi2',
                        # 'fParImpProd',
                        'fChi2PCA' ]
                       # list of training variables

    hyper_par: [
        {'max_depth':4, 'learning_rate':0.029, 'n_estimators':500, 'min_child_weight':2.7, 'subsample':0.90, 'colsample_bytree':0.97, 'n_jobs':8, 'tree_method':hist, 'lambda':0.01},
        {'max_depth':4, 'learning_rate':0.029, 'n_estimators':500, 'min_child_weight':2.7, 'subsample':0.90, 'colsample_bytree':0.97, 'n_jobs':8, 'tree_method':hist, 'lambda':0.01},
        {'max_depth':4, 'learning_rate':0.029, 'n_estimators':500, 'min_child_weight':2.7, 'subsample':0.90, 'colsample_bytree':0.97, 'n_jobs':8, 'tree_method':hist, 'lambda':0.01},
        {'max_depth':4, 'learning_rate':0.029, 'n_estimators':500, 'min_child_weight':2.7, 'subsample':0.90, 'colsample_bytree':0.97, 'n_jobs':8, 'tree_method':hist, 'lambda':0.01},
        {'max_depth':4, 'learning_rate':0.029, 'n_estimators':500, 'min_child_weight':2.7, 'subsample':0.90, 'colsample_bytree':0.97, 'n_jobs':8, 'tree_method':hist, 'lambda':0.01},
        # {'max_depth':4, 'learning_rate':0.029, 'n_estimators':500, 'min_child_weight':2.7, 'subsample':0.90, 'colsample_bytree':0.97, 'n_jobs':8, 'tree_method':hist, 'lambda':0.01}
    ]

    # hyper_par: [{'max_depth':5, 'learning_rate':0.11, 'n_estimators':807, 'min_child_weight':5.12, 'colsample':0.96, 'n_jobs':4, 'tree_method':hist},
    #             {'max_depth':4, 'learning_rate':0.11, 'n_estimators':807, 'min_child_weight':5, 'colsample':0.9, 'n_jobs':4, 'tree_method':hist},
    #             {'max_depth':4, 'learning_rate':0.12, 'n_estimators':807, 'min_child_weight':5, 'colsample':0.9, 'n_jobs':4, 'tree_method':hist},
    #             {'max_depth':4, 'learning_rate':0.11, 'n_estimators':806, 'min_child_weight':5, 'colsample':0.9, 'n_jobs':4, 'tree_method':hist},
    #             {'max_depth':3, 'learning_rate':0.085, 'n_estimators':650, 'min_child_weight':5, 'colsample':0.9, 'n_jobs':4, 'tree_method':hist},
    #             {'max_depth':2, 'learning_rate':0.04, 'n_estimators':500, 'min_child_weight':5, 'colsample':0.9, 'n_jobs':4, 'tree_method':hist}]
                # list of dicts of hyperparameters (one for each pT bin)

    hyper_par_opt:
      do_hyp_opt: False # whether to do the parameter optimization
      njobs: -1 # number of parallel jobs used in hyper-parameter optimization, -1. to use all
      nfolds: 5 # number of folds used in cross validation
      initpoints: 5 # steps of random exploration you want to perform
      niter: 5 # steps for bayesian optimization
      ntrials: 5
      direction: 'maximize'
      bayes_opt_config: {
                        # 'max_depth': [2, 6],
                        'learning_rate': [0.005, 0.3],
                        'n_estimators': [50, 700],
                        'min_child_weight': [1, 15],
                        'subsample': [0.7, 1.],
                        'colsample_bytree': [0.7, 1.]}
                        # configuration dictionary for optimize_params_bayes()
    
    saved_models: [    /home/mdicosta/flowDplus/MLtraining/opt10trials_maxsgn_test01_dcacuts_DLcuts_bkgmult5/pt1_3/ModelHandler_pT_1_3.pickle,
                       /home/mdicosta/flowDplus/MLtraining/opt10trials_maxsgn_test01_dcacuts_DLcuts_bkgmult5/pt3_5/ModelHandler_pT_3_5.pickle,
                       /home/mdicosta/flowDplus/MLtraining/opt10trials_maxsgn_test01_dcacuts_DLcuts_bkgmult5/pt5_8/ModelHandler_pT_5_8.pickle,
                       /home/mdicosta/flowDplus/MLtraining/opt10trials_maxsgn_test01_dcacuts_DLcuts_bkgmult5/pt8_12/ModelHandler_pT_8_12.pickle,
                       /home/mdicosta/flowDplus/MLtraining/opt10trials_maxsgn_test01_dcacuts_DLcuts_bkgmult5/pt12_24/ModelHandler_pT_12_24.pickle
                   ] # list of saved ModelHandler (path+file), compatible with the pt bins

plots:
    plotting_columns: [ 'fM', 'fPt', 'fPhi', 'fDecayLength', 'fDecayLengthXY', 
                        'fCpa', 'fCpaXY', 'fImpactParameter0', 'fImpactParameter1', 
                        'fImpactParameter2', 'fNSigTpcTofPi0',   'fNSigTpcTofKa1', 
                        'fNSigTpcTofPi2',
                        # , 'fParImpProd'] #, 
                        'fChi2PCA' ]
                       # list of variables to plot
    train_test_log: True # use log scale for plots of train and test distributions
  
appl: 
    column_to_save_list: ['fM', 'fPt'] # list of variables saved in the dataframes with the applied models

savecands: true
standalone_appl:
    treename: null # null if using parquet
    inputs: [] # list of parquet files for the model application
    output_names: [] # names for the outputs (one for each file)
    output_dir: null # output directory
